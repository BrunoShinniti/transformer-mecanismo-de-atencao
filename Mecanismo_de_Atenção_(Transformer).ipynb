{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kAG0LIjJvypI"
      ],
      "authorship_tag": "ABX9TyOxAgWGlT3Vbv5CuN1GX4/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoShinniti/transformer-mecanismo-de-atencao/blob/main/Mecanismo_de_Aten%C3%A7%C3%A3o_(Transformer).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## com PyTorch"
      ],
      "metadata": {
        "id": "kAG0LIjJvypI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo9PNkd0cTI-",
        "outputId": "27e4bb83-5d2e-48d0-f9b4-8d110ac0d2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "r_g0v5MDcdrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, n_heads, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.drouptout = dropout\n",
        "\n",
        "        # Define a camada de embedding que transformará a sequência de entrada em uma sequência de vetores de dimensão fixa\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Define o mecanismo de auto-atenção multi-head\n",
        "        self.attention = nn.MultiheadAttention(embedding_dim, n_heads, dropout = dropout)\n",
        "\n",
        "        # Define a rede neural feed-forward qe será usada para gerar a sequência de saída a partir da sequência de entrada\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "\n",
        "        # Define a camada de saída final que transformará a sequência de saída na forma de saída desejada\n",
        "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Aplica a camada de embedding à sequência de entrada\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Aplica o mecanismo multi-headed self-attetion\n",
        "        x = self.attention(x)\n",
        "\n",
        "        # Aplica o método feed-forward\n",
        "        x = self.feed_forward(x)\n",
        "\n",
        "        # Aplica camada final\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "modelo = Transformer(vocab_size = 1000,\n",
        "                     embedding_dim = 32,\n",
        "                     n_heads = 4,\n",
        "                     n_layers = 2,\n",
        "                     dropout = 0.5)\n",
        "\n",
        "modelo.modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS-MnpHbcdl-",
        "outputId": "14453091-88d4-43df-81de-1658e32ba85d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of Transformer(\n",
              "  (embedding): Embedding(1000, 32)\n",
              "  (attention): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "  )\n",
              "  (feed_forward): Sequential(\n",
              "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
              "  )\n",
              "  (out): Linear(in_features=32, out_features=1000, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMdsPmrCcdg-",
        "outputId": "3cc3f75c-4b91-4c79-ca71-a327db5c5840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiheadAttention(\n",
              "  (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sem Framework\n",
        "\n",
        "Construção de modelo capaz de prever sequências de comprimento igual a 10 tokens.\n",
        "\n",
        "* 1 - Camada de Embedding: Transforma as palavras em vetores numéricos de tamanho fixo.\n",
        "\n",
        "* 2 - Mecanismo de Atenção: Permite que o modelo foque em diferentes partes da entrada.\n",
        "\n",
        "* 3 - Camadas Encoder e Decoder: Processam os dados sequencialmente\n",
        "\n",
        "* 4 - Camada Linear e Softmax: Para predições finais\n",
        "\n",
        "#### Objetivo:\n",
        "Implementar o item 2, mas para deixar o exemplo funcional, será implementado os itens 1 e 4."
      ],
      "metadata": {
        "id": "vxosJYhQvwYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hiperparâmetros Iniciais"
      ],
      "metadata": {
        "id": "fgTydZXUwkxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "4Id7O91FcdeH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensão do modelo\n",
        "dim_model = 64\n",
        "\n",
        "# Comprimento da sequência\n",
        "seq_lenght = 10\n",
        "\n",
        "# Tamanho do vocabulário\n",
        "vocab_size = 100"
      ],
      "metadata": {
        "id": "T8EI6eS4cdba"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Camada Embedding\n",
        "\n",
        "A função embedding é utulizada para converter entradas sequenciais em vetores densos de tamanho fixo. Esses vetores são conhecimdos como embeddings e são uma parte fundamental, em especial dos modelos PLN.<br><br>\n",
        "Esses embeddings são fundamentais para modelos de aprendizado profundo em PLN, pois fornecem uma representação rica e densa de palavras ou tokens, capturando informações contextuais e semânticas que são essenciais para tarefas como tradução automática, classificação de texto, entre outras."
      ],
      "metadata": {
        "id": "qH5fgUs60RrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(input, vocab_size, dim_model):\n",
        "\n",
        "    # Cria uma matriz de embedding onde cada linha representa um token de vocabulário\n",
        "    # A matriz é inicializada com valores aleatórios normalmente distrbuídos\n",
        "    embed = np.random.randn(vocab_size, dim_model)\n",
        "\n",
        "    return np.array([embed[i] for i in input])"
      ],
      "metadata": {
        "id": "G2s9JlTvcdYn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax\n",
        "A função softmax é uma função de ativação amplamente utilizada em redes neurais, especialmente em cenários de classificação, onde é importante transformar valores brutos de saída (logits) em probabilidades que somam 1. Abaixo, está o código da função softmax com comentários em cada linha explicando seu funcionamento:"
      ],
      "metadata": {
        "id": "Ckwh1zTFJek9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "\n",
        "    # Calcula o exponencial de cada elemtno do input, ajustado pelo máximo valor no input para evitar overflow numérico\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "\n",
        "    # Divide cada exponencial pelo somatório dos exponenciais ao longo do último eixo (axis = -1)\n",
        "    # O Reshape(-1, 1) garante que a divisão seja realizada corretamente em um contexto multidimensional\n",
        "    return e_x / e_x.sum(axis=-1).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "QdygZXJTcdQl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale dot product\n",
        "\n",
        "A função scaled_dot_product_attention() é um componente do mecanismo de atenção em modelos Transformer. Ela calcula a atenção entre conjuntos de queries (Q), keys (K) e values (V).\n",
        "\n",
        "Essencialmente, essa função permite que o modelo dê importância diferenciada a diferentes partes da entrada, um aspecto chave que torna os modelos Transformer particularmente eficazes para tarefas de PLN e outras tarefas sequenciais."
      ],
      "metadata": {
        "id": "ea80hJEf778v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a função para calcular a atenção escalada por produto escalar\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "\n",
        "    # Calcula o produto escalar entre Q e a transposta de K\n",
        "    matmul_qk = np.dot(Q, K.T)\n",
        "\n",
        "    # Obtém a dimensão dos vetores de chave\n",
        "    depth = K.shape[-1]\n",
        "\n",
        "    # Escala os logits dividindo-os pela raiz quadrada de profundidade\n",
        "    logits = matmul_qk / np.sqrt(depth)\n",
        "\n",
        "    # Aplica a função softmax para obter os pesos de atenção\n",
        "    attention_weights = softmax(logits)\n",
        "\n",
        "    # Multiplica os pesos de atenção pelos valores V para obter a saída final\n",
        "    output = np.dot(attention_weights, V)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "rPnXnumFcdF-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saída do Modelo com operação Linear e Softmax\n",
        "\n",
        "A função linear_and_softmax( ) é uma combinação de uma camada linear seguida por uma função softmax, comumente usada em modelos de aprendizado profundo, especialmente em tarefas de classificação."
      ],
      "metadata": {
        "id": "xyxO8u90DJ8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a função que aplica uma transformação linear seguida de softmax\n",
        "\n",
        "def linear_and_softmax(input):\n",
        "\n",
        "    # Inicializa uma matriz de pesos com valores aleatórios normamente distribuídos\n",
        "    # Esta matriz conecta cada dimensão do modelo (dim_model)  a cada palavra do vocabulário (vocab_size)\n",
        "    weights = np.random.randn(dim_model, vocab_size)\n",
        "\n",
        "    # Realiza a operação linear (produto escalar) entre a entrada e a matriz de pesos\n",
        "    # O resultado, logits, é um vetor que representa a entrada transformada em um espaço de maior dimensão\n",
        "    logits = np.dot(input, weights)\n",
        "\n",
        "    # Aplica a função softmax aos logits\n",
        "    # Isso transforma os logits em um vetor de probabilidades, onde cada elemento soma 1\n",
        "\n",
        "    return softmax(logits)"
      ],
      "metadata": {
        "id": "uLhiyjF0cdDm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo Final"
      ],
      "metadata": {
        "id": "Aus67xauKQvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_model(input):\n",
        "\n",
        "    # Embedding\n",
        "    embedded_input = embedding(input, vocab_size, dim_model)\n",
        "\n",
        "    # Mecanismo de atenção\n",
        "    attention_output = scaled_dot_product_attention(embedded_input, embedded_input, embedded_input)\n",
        "\n",
        "    # Camada linear e softmax\n",
        "    output_probabilities = linear_and_softmax(attention_output)\n",
        "\n",
        "    # Escolhendo os índices com maior probabilidade\n",
        "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
        "\n",
        "    return output_indices"
      ],
      "metadata": {
        "id": "iF8qrnPGcdBa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando modelo para previsão"
      ],
      "metadata": {
        "id": "zshLqTtpKzPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = np.random.randint(0, vocab_size, seq_lenght)\n",
        "input_sequence"
      ],
      "metadata": {
        "id": "6mqFd-QUcc_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2b0906-8aa2-4b9d-f5dd-ae86d0dc93fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([37, 97, 12,  5, 54, 59, 68, 20, 33, 34])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = transformer_model(input_sequence)"
      ],
      "metadata": {
        "id": "xZ9hHU4Bcc6X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "RboOJuvncc4G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ae3f32-7fec-460d-fc7c-8efd2758587a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 31, 87, 64, 31, 79, 25, 60, 12, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}